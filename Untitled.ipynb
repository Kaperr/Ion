{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n",
    "                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,\n",
    "                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D, Layer)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, get_custom_objects\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import constraints\n",
    "import tensorflow as tf\n",
    "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n",
    "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from contextlib import contextmanager\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "import tensorflow_addons as tfa\n",
    "import scipy.stats as stats\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "import pywt\n",
    "import os\n",
    "import gc\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=120\n",
    "NNBATCHSIZE=20\n",
    "BATCHSIZE = 4000\n",
    "SEED = 42\n",
    "SELECT = True\n",
    "SPLITS = 5\n",
    "LR = 0.001\n",
    "fe_config = [(True, 4000),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name : Text):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "COMPETITION = 'ION-Switching'\n",
    "logger = getLogger(COMPETITION)\n",
    "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "MODELNAME = 'WaveNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel(INFO)\n",
    "    handler.setFormatter(Formatter(LOGFORMAT))\n",
    "    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n",
    "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
    "    logger.setLevel(INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.addHandler(fh_handler)\n",
    "    \n",
    "def seed_everything(seed : int) -> NoReturn :\n",
    "    \n",
    "    rn.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "def batching(df : pd.DataFrame,\n",
    "             batch_size : int) -> pd.DataFrame :\n",
    "    \n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float16).min\n",
    "                        and c_max < np.finfo(np.float16).max):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df\n",
    "\n",
    "def lag_with_pct_change(df : pd.DataFrame,\n",
    "                        shift_sizes : Optional[List]=[1, 2],\n",
    "                        add_pct_change : Optional[bool]=False,\n",
    "                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n",
    "    \n",
    "    for shift_size in shift_sizes:    \n",
    "        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n",
    "        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n",
    "\n",
    "    if add_pct_change:\n",
    "        df['pct_change'] = df['signal'].pct_change()\n",
    "        if add_pct_change_lag:\n",
    "            for shift_size in shift_sizes:    \n",
    "                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n",
    "                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n",
    "    return df\n",
    "\n",
    "def run_feat_enginnering(df : pd.DataFrame,\n",
    "                         create_all_data_feats : bool,\n",
    "                         batch_size : int) -> pd.DataFrame:\n",
    "    feature_selection\n",
    "    df = batching(df, batch_size=batch_size)\n",
    "    if create_all_data_feats:\n",
    "        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False)\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "def feature_selection(df : pd.DataFrame,\n",
    "                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n",
    "    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
    "    for col in use_cols:\n",
    "        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n",
    "        df[col] = df[col].fillna(col_mean)\n",
    "        df_test[col] = df_test[col].fillna(col_mean)\n",
    "   \n",
    "    gc.collect()\n",
    "    return df, df_test, use_cols\n",
    "\n",
    "def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    X = np.vstack((X, np.flip(X, axis=1)))\n",
    "    y = np.vstack((y, np.flip(y, axis=1)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def run_cv_model_by_batch(train : pd.DataFrame,\n",
    "                          test : pd.DataFrame,\n",
    "                          splits : int,\n",
    "                          batch_col : Text,\n",
    "                          feats : List,\n",
    "                          sample_submission: pd.DataFrame,\n",
    "                          nn_epochs : int,\n",
    "                          nn_batch_size : int) -> NoReturn:\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    oof_ = np.zeros((len(train), 11))\n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])    \n",
    "        new_splits.append(new_split)\n",
    "        \n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
    "\n",
    "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_'+str(i) for i in range(11)]\n",
    "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "\n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
    "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
    "        \n",
    "        if n_fold < 2:\n",
    "            train_x, train_y = augment(train_x, train_y)\n",
    "\n",
    "        gc.collect()\n",
    "        shape_ = (None, train_x.shape[2])\n",
    "        model = Classifier(shape_)\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, show_epoch_progress=False,show_overall_progress=True)\n",
    "        model.fit(train_x,train_y,\n",
    "                  epochs=nn_epochs,\n",
    "                  callbacks=[cb_prg, cb_lr_schedule, MacroF1(model, valid_x,valid_y)],\n",
    "                  batch_size=nn_batch_size,verbose=0,\n",
    "                  validation_data=(valid_x,valid_y))\n",
    "        preds_f = model.predict(valid_x)\n",
    "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n",
    "        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_orig_idx,:] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n",
    "    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
    "    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n",
    "    display(sample_submission.head())\n",
    "    np.save('oof.npy', oof_)\n",
    "    np.save('preds.npy', preds_)\n",
    "\n",
    "    return \n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 40:\n",
    "        lr = LR\n",
    "    elif epoch < 50:\n",
    "        lr = LR / 3\n",
    "    elif epoch < 60:\n",
    "        lr = LR / 6\n",
    "    elif epoch < 75:\n",
    "        lr = LR / 9\n",
    "    elif epoch < 85:\n",
    "        lr = LR / 12\n",
    "    elif epoch < 100:\n",
    "        lr = LR / 15\n",
    "    else:\n",
    "        lr = LR / 50\n",
    "    return lr\n",
    "\n",
    "class Mish(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Mish, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * K.tanh(K.softplus(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Mish, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "def mish(x):\n",
    "    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n",
    " \n",
    "get_custom_objects().update({'mish': Activation(mish)})\n",
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"Multi-headed attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, \n",
    "                 num_heads = 8, \n",
    "                 attention_dropout=.1,\n",
    "                 trainable=True,\n",
    "                 name='Attention'):\n",
    "        \n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.trainable = trainable\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n",
    "        super(Attention, self).__init__(name=name)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split x into different heads, and transpose the resulting value.\n",
    "        The tensor is transposed to insure the inner dimensions hold the correct\n",
    "        values during the matrix multiplication.\n",
    "        Args:\n",
    "          x: A tensor with shape [batch_size, length, hidden_size]\n",
    "        Returns:\n",
    "          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"split_heads\"):\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            length = tf.shape(x)[1]\n",
    "\n",
    "            # Calculate depth of last dimension after it has been split.\n",
    "            depth = (self.hidden_size // self.num_heads)\n",
    "\n",
    "            # Split the last dimension\n",
    "            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
    "\n",
    "            # Transpose the result\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine tensor that has been split.\n",
    "        Args:\n",
    "          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "        Returns:\n",
    "          A tensor with shape [batch_size, length, hidden_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"combine_heads\"):\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            length = tf.shape(x)[2]\n",
    "            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
    "            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply attention mechanism to inputs.\n",
    "        Args:\n",
    "          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n",
    "        Returns:\n",
    "          Attention layer output with shape [batch_size, length_x, hidden_size]\n",
    "        \"\"\"\n",
    "        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n",
    "        q = self.dense(inputs)\n",
    "        k = self.dense(inputs)\n",
    "        v = self.dense(inputs)\n",
    "\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        # Scale q to prevent the dot product between q and k from growing too large.\n",
    "        depth = (self.hidden_size // self.num_heads)\n",
    "        q *= depth ** -0.5\n",
    "        \n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        # logits += self.bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        \n",
    "        if self.trainable:\n",
    "            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n",
    "        \n",
    "        attention_output = tf.matmul(weights, v)\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        attention_output = self.dense(attention_output)\n",
    "        return attention_output\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(input_shape)\n",
    "    \n",
    "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha*((1-p)^gamma)*log(p)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        # Clip the prediction value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true*K.log(y_pred)\n",
    "        # Calculate weight that consists of  modulating factor and weighting factor\n",
    "        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n",
    "\n",
    "    def build_residual_block(l_input):\n",
    "        resid_input = l_input\n",
    "        for dilation_rate in [2**i for i in range(stacked_layer)]:\n",
    "            l_sigmoid_conv1d = Conv1D(\n",
    "              num_filters, kernel_size, dilation_rate=dilation_rate,\n",
    "              padding='same', activation='sigmoid')(l_input)\n",
    "            l_tanh_conv1d = Conv1D(\n",
    "             num_filters, kernel_size, dilation_rate=dilation_rate,\n",
    "             padding='same', activation='mish')(l_input)\n",
    "            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
    "            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n",
    "            resid_input = Add()([resid_input ,l_input])\n",
    "        return resid_input\n",
    "    return build_residual_block\n",
    "\n",
    "def Classifier(shape_):\n",
    "    num_filters_ = 16\n",
    "    kernel_size_ = 3\n",
    "    stacked_layers_ = [12, 8, 4, 1]\n",
    "    l_input = Input(shape=(shape_))\n",
    "    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
    "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
    "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
    "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
    "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
    "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
    "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
    "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
    "    l_output = Dense(11, activation='softmax')(x)\n",
    "    model = models.Model(inputs=[l_input], outputs=[l_output])\n",
    "    opt = Adam(lr=LR)\n",
    "    opt = tfa.optimizers.SWA(opt)\n",
    "    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def Classifierx(shape_):        \n",
    "    \n",
    "    inp = Input(shape=(shape_))\n",
    "    x = Bidirectional(GRU(256, return_sequences=True))(inp)\n",
    "    x = Attention(512)(x)\n",
    "    x = TimeDistributed(Dense(256, activation='mish'))(x)\n",
    "    x = TimeDistributed(Dense(128, activation='mish'))(x)\n",
    "    out = TimeDistributed(Dense(11, activation='softmax', name='out'))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out) \n",
    "    \n",
    "    opt = Adam(lr=LR)\n",
    "    opt = tfa.optimizers.SWA(opt)\n",
    "    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class MacroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average=\"macro\")\n",
    "        print(f' F1Macro: {score:.5f}')   \n",
    "        \n",
    "def normalize(train, test):\n",
    "    \n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
    "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def run_everything(fe_config : List) -> NoReturn:\n",
    "    not_feats_cols = ['time']\n",
    "    target_col = ['open_channels']\n",
    "    init_logger()\n",
    "    with timer(f'Reading Data'):\n",
    "        logger.info(f'Reading Data Started ...')\n",
    "        train = pd.read_csv('train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('test.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sample_submission = pd.read_csv('sample_submission.csv', dtype={'time': np.float32})\n",
    "    train, test = normalize(train, test)\n",
    "    logger.info('Reading and Normalizing Data Completed ...')\n",
    "    with timer(f'Creating Features'):\n",
    "        logger.info(f'Feature Enginnering Started ...')\n",
    "        for config in fe_config:\n",
    "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
    "            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n",
    "        train, test, feats = feature_selection(train, test)\n",
    "        logger.info(f'Feature Enginnering Completed ...')\n",
    "\n",
    "    with timer(f'Running Wavenet model'):\n",
    "        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n",
    "        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n",
    "        logger.info(f'Training completed ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:14,930 INFO Reading Data Started ...\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:17,399 INFO [Reading Data] done in 2 s\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,129 INFO Reading and Normalizing Data Completed ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:19,141 INFO Feature Enginnering Started ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,608 INFO Feature Enginnering Completed ...\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,618 INFO [Creating Features] done in 12 s\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n",
      "2020-04-09 22:22:31,629 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67456fdce2f24df98a6b720b64868f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', layout=Layout(flex='2'), max=120.0, style=Progâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kaper\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      " F1Macro: 0.27717\n",
      " F1Macro: 0.37509\n",
      " F1Macro: 0.47783\n"
     ]
    }
   ],
   "source": [
    "run_everything(fe_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
